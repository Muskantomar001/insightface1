# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gzfYT0JNscEQVaBcSeAcbn6U85W_UcMF
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/My Drive/DLDatasets")
!ls

!git clone https://github.com/foamliu/InsightFace-v2.git

!pip install matplotlib
!pip install scipy
!pip install tqdm
!pip install opencv-python
!pip install pillow
!pip install torch
!pip install torchvision
!pip install numpy
!pip install scikit-image
!pip install torchsummary
!pip install imgaug

!pip install face-alignment

import pandas as pd

!pip install -q torch==1.5.0 torchvision

import torch
print(torch.__version__)

!apt-get install python3.6

!pip install utils

!pip install config

!pip install mtcnn

!pip install mxnet

#extract.py
import os

import zipfile


def extract(filename):
    print('Extracting {}...'.format(filename))
    zip_ref = zipfile.ZipFile(filename, 'r')
    zip_ref.extractall('data')
    zip_ref.close()


if __name__ == "__main__":
    # if not os.path.isdir('data/faces_ms1m_112x112'):
    #     extract('data/faces_ms1m_112x112.zip')
    if not os.path.isdir('/content/msceleb.'):
        extract('/content/msceleb.zip')

#pre_process.py
import os
import pickle

import cv2 as cv
import mxnet as mx
from mxnet import recordio
from tqdm import tqdm

from config import path_imgidx, path_imgrec, IMG_DIR, pickle_file
from utils import ensure_folder

if __name__ == "__main__":
    ensure_folder(IMG_DIR)
    imgrec = recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, 'r')
    print(len(imgrec))

    samples = []
    class_ids = set()

    # %% 1 ~ 5179510
    for i in tqdm(range(5179510)):
        print(i)
        try:
            header, s = recordio.unpack(imgrec.read_idx(i + 1))
            img = mx.image.imdecode(s).asnumpy()
            print(img.shape)
            img = cv.cvtColor(img, cv.COLOR_RGB2BGR)
            print(header.label)
            print(type(header.label))
            label = int(header.label[0])
            class_ids.add(label)
            filename = '{}.jpg'.format(i)
            samples.append({'img': filename, 'label': label})
            filename = os.path.join(IMG_DIR, filename)
            cv.imwrite(filename, img)
        except KeyboardInterrupt:
            raise
        except Exception as err:
            print(err)
            print(i)
            print(label)
            # pass

    with open(pickle_file, 'wb') as file:
        pickle.dump(samples, file)

    print('num_samples: ' + str(len(samples)))

    class_ids = list(class_ids)
    print(len(class_ids))
    print(max(class_ids))

!cp "/content/config.py" .

!pip install face-alignment

!pip install opencv-python

import cv2
# print version number
print(cv2.__version__)

!git clone https://github.com/Muskantomar001/insightface1.git

#demo.py
import cv2 as cv
import numpy as np
from PIL import Image


from visualization_utils import show_bboxes

if __name__ == '__main__':
    img = Image.open('images/office1.jpg')
    bounding_boxes, landmarks = detector.detect_faces(img)
    img = cv.cvtColor(np.array(img), cv.COLOR_RGB2BGR)
    show_bboxes(img, bounding_boxes, landmarks)

    img = Image.open('images/office2.jpg')
    bounding_boxes, landmarks = detector.detect_faces(img)
    img = cv.cvtColor(np.array(img), cv.COLOR_RGB2BGR)
    show_bboxes(img, bounding_boxes, landmarks)

    img = Image.open('images/office3.jpg')
    bounding_boxes, landmarks = detector.detect_faces(img)
    img = cv.cvtColor(np.array(img), cv.COLOR_RGB2BGR)
    show_bboxes(img, bounding_boxes, landmarks)

    img = Image.open('images/office4.jpg')
    bounding_boxes, landmarks = detector.detect_faces(img)
    img = cv.cvtColor(np.array(img), cv.COLOR_RGB2BGR)
    show_bboxes(img, bounding_boxes, landmarks)

    img = Image.open('images/office5.jpg')
    bounding_boxes, landmarks = detector.detect_faces(img)
    img = cv.cvtColor(np.array(img), cv.COLOR_RGB2BGR)
    show_bboxes(img, bounding_boxes, landmarks)

!cp "/content/visualization_utils.py" .

from mtcnn.mtcnn import MTCNN

detector = MTCNN()

!pip install tensorflow

import cv2


def show_bboxes(img, bounding_boxes, facial_landmarks=[]):
    """Draw bounding boxes and facial landmarks.
    Arguments:
        img: an instance of PIL.Image.
        bounding_boxes: a float numpy array of shape [n, 5].
        facial_landmarks: a float numpy array of shape [n, 10].
    Returns:
        an instance of PIL.Image.
    """

    # img_copy = img.copy()
    # draw = ImageDraw.Draw(img_copy)
    draw = img.copy()

    for b in bounding_boxes:
        b = [int(round(value)) for value in b]
        # print (b)
        cv2.rectangle(draw, (b[0], b[1]), (b[2], b[3]), (0,255,0), 2)
        

    for p in facial_landmarks:
        for i in range(5):
            cv2.circle(draw, (p[i] , p[i + 5]), 1, (255,0,0), -1)
        
    return draw

!cp "/content/box_utils.py" .

import box_utils

!cp "/content/models.py" .

#mtcnn
#detector.py
import numpy as np
import torch
from torch.autograd import Variable

from box_utils import nms, calibrate_box, convert_to_square
from first_stage import run_first_stage
from models import PNet, RNet, ONet

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def detect_faces(image, min_face_size=20.0,
                 thresholds=[0.6, 0.7, 0.8],
                 nms_thresholds=[0.7, 0.7, 0.7]):
    """
    Arguments:
        image: an instance of PIL.Image.
        min_face_size: a float number.
        thresholds: a list of length 3.
        nms_thresholds: a list of length 3.
    Returns:
        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],
        bounding boxes and facial landmarks.
    """

    with torch.no_grad():
        # LOAD MODELS
        pnet = PNet().to(device)
        rnet = RNet().to(device)
        onet = ONet().to(device)
        onet.eval()

        # BUILD AN IMAGE PYRAMID
        width, height = image.size
        min_length = min(height, width)

        min_detection_size = 12
        factor = 0.707  # sqrt(0.5)

        # scales for scaling the image
        scales = []

        # scales the image so that
        # minimum size that we can detect equals to
        # minimum face size that we want to detect
        m = min_detection_size / min_face_size
        min_length *= m

        factor_count = 0
        while min_length > min_detection_size:
            scales.append(m * factor ** factor_count)
            min_length *= factor
            factor_count += 1

        # STAGE 1

        # it will be returned
        bounding_boxes = []

        # run P-Net on different scales
        for s in scales:
            boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])
            bounding_boxes.append(boxes)

        # collect boxes (and offsets, and scores) from different scales
        bounding_boxes = [i for i in bounding_boxes if i is not None]
        bounding_boxes = np.vstack(bounding_boxes)

        keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])
        bounding_boxes = bounding_boxes[keep]

        # use offsets predicted by pnet to transform bounding boxes
        bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])
        # shape [n_boxes, 5]

        bounding_boxes = convert_to_square(bounding_boxes)
        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])

        # STAGE 2

        img_boxes = get_image_boxes(bounding_boxes, image, size=24)
        img_boxes = Variable(torch.FloatTensor(img_boxes).to(device))
        output = rnet(img_boxes)
        offsets = output[0].data.cpu().numpy()  # shape [n_boxes, 4]
        probs = output[1].data.cpu().numpy()  # shape [n_boxes, 2]

        keep = np.where(probs[:, 1] > thresholds[1])[0]
        bounding_boxes = bounding_boxes[keep]
        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))
        offsets = offsets[keep]

        keep = nms(bounding_boxes, nms_thresholds[1])
        bounding_boxes = bounding_boxes[keep]
        bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])
        bounding_boxes = convert_to_square(bounding_boxes)
        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])

        # STAGE 3

        img_boxes = get_image_boxes(bounding_boxes, image, size=48)
        if len(img_boxes) == 0:
            return [], []
        img_boxes = Variable(torch.FloatTensor(img_boxes).to(device))
        output = onet(img_boxes)
        landmarks = output[0].data.cpu().numpy()  # shape [n_boxes, 10]
        offsets = output[1].data.cpu().numpy()  # shape [n_boxes, 4]
        probs = output[2].data.cpu().numpy()  # shape [n_boxes, 2]

        keep = np.where(probs[:, 1] > thresholds[2])[0]
        bounding_boxes = bounding_boxes[keep]
        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))
        offsets = offsets[keep]
        landmarks = landmarks[keep]

        # compute landmark points
        width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0
        height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0
        xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]
        landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1) * landmarks[:, 0:5]
        landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1) * landmarks[:, 5:10]

        bounding_boxes = calibrate_box(bounding_boxes, offsets)
        keep = nms(bounding_boxes, nms_thresholds[2], mode='min')
        bounding_boxes = bounding_boxes[keep]
        landmarks = landmarks[keep]

        return bounding_boxes, landmarks



!cp "/content/first_stage.py" .

import first_stage